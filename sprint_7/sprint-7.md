# Sprint 7

<img src="https://github.com/Jpedro900/compass_repo_joao_pedro_souza/assets/127545539/f1b4aec6-e972-41e2-a8df-aa13efdc4e67" width=100px><img src="https://github.com/Jpedro900/compass_repo_joao_pedro_souza/assets/127545539/2116b3ce-a5e8-43dc-bfb8-0c29228f3a30" width=100px>

# Hadoop e PySpark

Durante este segundo sprint, mergulhei em uma jornada de aprendizado intensiva, explorando duas tecnologias fundamentais para o processamento e análise de Big Data: Hadoop e Spark.

No curso relacionado ao Hadoop, fui introduzido aos fundamentos dessa tecnologia, compreendendo como ela revolucionou a maneira como lidamos com grandes volumes de dados. Através de exemplos práticos e problemas reais, ganhei uma compreensão sólida de como aplicar conceitos de Hadoop para resolver desafios complexos de Big Data.

Em seguida, mergulhei na tecnologia Spark, uma ferramenta poderosa de processamento de dados em memória. Ao explorar o Spark, aprendi a manipular e analisar grandes conjuntos de dados de forma eficiente e escalável, utilizando a linguagem Pyspark.

Ao longo desses estudos, enfrentei desafios e obstáculos, mas cada dificuldade superada fortaleceu minha compreensão e habilidades nessas tecnologias. Compreendi que, embora os caminhos para dominar o Hadoop e o Spark possam ser longos e desafiadores, são os caminhos certos para alcançar resultados significativos na análise de Big Data.

Ao final desta jornada, pude perceber claramente os benefícios de utilizar tais tecnologias para organizar processos de trabalho e compreender o fluxo de dados na internet. Aprendi que o conhecimento e a aplicação de Hadoop e Spark são fundamentais para lidar com os desafios do Big Data e explorar novas oportunidades de insights e inovação em um mundo cada vez mais orientado por dados.
